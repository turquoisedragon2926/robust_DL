{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMU-5NWxiY-7",
        "outputId": "b9956b3f-5f66-4a25-e448-82a323f0184c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/robust_DL\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/MyDrive/robust_DL\n",
        "# %mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JJOyIqHsaLcH"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from losses.trades import trades_loss\n",
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from models.wideresnet import *\n",
        "from models.resnet import *\n",
        "from models.small_cnn import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from models.AlexNet import AlexNet\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NdePmTVOdfv0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch import linalg\n",
        "\n",
        "\n",
        "def log_prob_drichlet(x,theta):\n",
        "    D = torch.distributions.dirichlet.Dirichlet(theta)\n",
        "    return D.log_prob(x)\n",
        "\n",
        "\n",
        "def gaussian_noise_layer(data, severity=0.05):\n",
        "    # c = [0.04, 0.06, .08, .09, .10][severity - 1]\n",
        "    noise = torch.randn(data.size())* severity#.to(device)\n",
        "\n",
        "    noise =torch.Tensor(noise).to(device)\n",
        "    # noise = noise*data.mean()\n",
        "    # noisy_data = torch.clip(data + noise)\n",
        "    noisy_data = data+noise\n",
        "    # noise = noisy_data - data\n",
        "    noise_norm = linalg.vector_norm(noise, dim = (1,2,3)).reshape((-1,1))  # improve this line of code later\n",
        "    return noisy_data, noise_norm\n",
        "\n",
        "def adaptive_loss(model,x_natural,y,noise_model, severity=0.05):\n",
        "    logits = model(x_natural)\n",
        "    p = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    noisy_data, noise_norm = gaussian_noise_layer(x_natural)\n",
        "    noise_weight = noise_model(noise_norm)#torch.div(noise_model(noise_norm), noise_model(torch.zeros_like(noise_norm)))\n",
        "    y_one_hot = torch.zeros_like(logits)\n",
        "    y_one_hot[:,y]=1\n",
        "\n",
        "    # loss_noisy = log_prob_dirichlet(logits, noise_weight)\n",
        "    loss_noisy = log_prob_drichlet(p,noise_weight*y_one_hot+1).mean()\n",
        "\n",
        "    # loss_normal = F.cross_entropy(logits , y)\n",
        "    return loss_noisy\n",
        "\n",
        "def adaptive_loss_v2(model,x_natural,y,noise_model, severity=0.05, w_noise=0.5):\n",
        "    logits = model(x_natural)\n",
        "    p = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    noisy_data, noise_norm = gaussian_noise_layer(x_natural)\n",
        "    noise_weight = noise_model(noise_norm)#torch.div(noise_model(noise_norm), noise_model(torch.zeros_like(noise_norm)))\n",
        "    y_one_hot = torch.zeros_like(logits)\n",
        "    y_one_hot[:,y]=1\n",
        "\n",
        "    # loss_noisy = log_prob_dirichlet(logits, noise_weight)\n",
        "    loss_noisy = log_prob_drichlet(p,noise_weight*y_one_hot+1).mean()\n",
        "\n",
        "    loss_normal = F.cross_entropy(logits , y)\n",
        "    return (1-w)*loss_normal+w_noise*loss_noisy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E5VbKzmtaLcH"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "  def __init__(self, train_loader, valid_loader, test_loader, attack_loader):\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.attack_loader = attack_loader\n",
        "\n",
        "class Model:\n",
        "  model = None\n",
        "  def __init__(self, id):\n",
        "    self.id = id\n",
        "\n",
        "class Loss:\n",
        "  def __init__(self, loss_fn, id=None):\n",
        "    self.loss_fn = loss_fn\n",
        "    self.id = id\n",
        "\n",
        "class Configuration:\n",
        "  def __init__(self, data, model, noise_model, loss, attack, model_pt=None, id=None):\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.noise_model = noise_model\n",
        "    self.loss = loss\n",
        "    self.attack = attack\n",
        "    self.model_pt = model_pt # Should move this to model\n",
        "\n",
        "    self.id = id\n",
        "\n",
        "  def getConfig(self):\n",
        "    return self.data, self.model, self.noise_model, self.loss, self.attack\n",
        "\n",
        "  def getId(self):\n",
        "    return self.id\n",
        "\n",
        "class CIFAR10CDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "def general_trades_loss_fn(beta=6.0, epsilon=0.3, step_size=0.007, num_steps=10):\n",
        "  def trades_loss_fn(model, data, target, optimizer):\n",
        "    return trades_loss(model=model, x_natural=data, y=target, optimizer=optimizer, step_size=step_size,\n",
        "                      epsilon=epsilon, perturb_steps=num_steps, beta=beta, distance='l_inf')\n",
        "  return trades_loss_fn\n",
        "\n",
        "def general_adaptive_loss_fn(severity=0.05):\n",
        "  def adaptive_loss_fn(model, data, target,noise_model, severity):\n",
        "    return adaptive_loss(model,data,target,noise_model, severity=severity)\n",
        "\n",
        "  return adaptive_loss_fn\n",
        "\n",
        "def ce_loss_fn(model, data, target, optimizer):\n",
        "    return F.cross_entropy(model(data), target)\n",
        "\n",
        "def identity_attack(model, X, y):\n",
        "  out = model(X)\n",
        "  acc = (out.data.max(1)[1] == y.data).float().sum()\n",
        "  return acc.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "emkusoN_aLcI"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, data_loader, device):\n",
        "    print('EVAL')\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "def robust_accuracy(model, attack, data_loader, device):\n",
        "    print('ROBUST EVAL')\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in data_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
        "        correct_count = attack(model, X, y)\n",
        "        correct += correct_count\n",
        "        total += target.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "def train(model, data,noise_model,  loss, config, epochs, eval_interval, device):\n",
        "  print('TRAINING')\n",
        "  data_loader = data.train_loader\n",
        "  valid_loader = data.valid_loader\n",
        "  attack_loader = data.attack_loader\n",
        "\n",
        "  optimizer = optim.SGD(list(model.parameters())+list(noise_model.parameters()), lr=0.01, momentum=0.9)\n",
        "  # TODO: We can move the optimizer to a field of Loss object\n",
        "\n",
        "  model.to(device)\n",
        "  if config.model_pt is not None:\n",
        "    model.load_state_dict(torch.load(config.model_pt))\n",
        "\n",
        "  best_eval_acc = 0.0\n",
        "  patience = 5  # number of VAL Acc values observed after best value to stop training\n",
        "\n",
        "  # Initialize lists to store per-epoch loss and validation accuracy\n",
        "  epoch_losses = []\n",
        "  eval_accuracies = []\n",
        "\n",
        "  for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    noise_model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        l = loss.loss_fn(model, data, target, noise_model, severity=0.05)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += l.item()\n",
        "\n",
        "        print(loss.id + f\" @ EP={epoch} & Batch idx \" + str(batch_idx) + \" / \" + str(len(data_loader) - 1) + \" Loss: \" + str(l.item()))\n",
        "\n",
        "    epoch_losses.append(total_loss / len(data_loader))\n",
        "\n",
        "    if epoch == 1 or epoch % eval_interval == 0 or epoch == epochs:\n",
        "      eval_acc= accuracy(model, valid_loader, device)\n",
        "      eval_accuracies.append(eval_acc)\n",
        "\n",
        "      if (eval_acc > best_eval_acc):  # best so far so save checkpoint to restore later\n",
        "        best_eval_acc = eval_acc\n",
        "        patience_count = 0\n",
        "        torch.save(model.state_dict(), os.path.join(\"weights\", loss.id + \".pt\"))\n",
        "        torch.save(optimizer.state_dict(), os.path.join(\"optimizers\", loss.id +  \".tar\"))\n",
        "      else:\n",
        "          patience_count += 1\n",
        "\n",
        "    # Plotting the loss and accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.suptitle(loss.id)\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epoch_losses, label='Training Loss')\n",
        "    plt.title('Loss vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot validation accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(eval_accuracies, label='Validation Accuracy')\n",
        "\n",
        "    if epoch == epochs or patience_count >= patience:\n",
        "      # Get the CIFAR 10 C evaluation accuracy and plot the horizontal line\n",
        "      cifar10c_eval_acc = robust_accuracy(model, config.attack, attack_loader, device)\n",
        "      plt.axhline(y=cifar10c_eval_acc, color='r', linestyle='-', label='CIFAR 10 C EVAL')\n",
        "\n",
        "    plt.title('Validation Accuracy vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plots\n",
        "    if not os.path.exists('plots'):\n",
        "        os.makedirs('plots')\n",
        "    plt.savefig(os.path.join('plots', loss.id + '_training_validation_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    if patience_count >= patience:\n",
        "      print(f\"Early Stopping!, epoch {epoch}\")\n",
        "      break\n",
        "\n",
        "  return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxEBl4HpaLcI",
        "outputId": "bd130bb4-1e8a-4217-d4e2-eba613060bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "num_epoch=2\n",
        "valid_size=0.2\n",
        "eval_interval=1\n",
        "transform_train = transforms.Compose([\n",
        "transforms.RandomCrop(32, padding=4),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.ToTensor(),])\n",
        "transform_test = transforms.Compose([\n",
        "transforms.ToTensor(),])\n",
        "use_cuda = torch.cuda.is_available()\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "transform_attack = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform_train)\n",
        "validset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform_test)\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "\n",
        "cifar10_train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, sampler=train_sampler, **kwargs)\n",
        "cifar10_valid_loader = torch.utils.data.DataLoader(trainset , batch_size=128, sampler=valid_sampler, **kwargs)\n",
        "cifar10_test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, **kwargs)\n",
        "\n",
        "\n",
        "transform_cifar10c = transforms.Compose([\n",
        "transforms.ToPILImage(),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.5,), (0.5,))])\n",
        "images = np.load('data/CIFAR-10-C/spatter.npy') # Set this to whatever we want\n",
        "labels = np.load('data/CIFAR-10-C/labels.npy')\n",
        "cifar10c_dataset = CIFAR10CDataset(data=images,labels=labels,transform=transform_cifar10c)\n",
        "cifar10c_attack_loader = DataLoader(cifar10c_dataset, batch_size=200, shuffle=False)\n",
        "\n",
        "cifar10_c_data = Data(cifar10_train_loader, cifar10_valid_loader,cifar10_test_loader, cifar10c_attack_loader)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "configurations = []\n",
        "final_loss = {}\n",
        "natural_accuracy = {}\n",
        "robustness_accuracy = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQLOW38MaLcI",
        "outputId": "5a369071-c6af-4bec-c872-8d7f2e0224a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Loss at 0x7f38592a4af0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "alpha=5\n",
        "beta = 1 / alpha\n",
        "severity=0.05\n",
        "id = f'CIFARC10:Alexnet:TRADES_LOSS:BETA={beta}'\n",
        "model_pt = None # os.path.join(\"weights\", f'CIFARC10:RESNET18:TRADES_LOSS:BETA={beta}_ep=0.pt')\n",
        "\n",
        "alexnet = AlexNet().to(device)\n",
        "noise_model = nn.Sequential(nn.Linear(1,1, bias=True),nn.Sigmoid()).to(device)\n",
        "trades_loss_beta = Loss(general_trades_loss_fn(beta=beta), id)\n",
        "adaptive_loss_severity=Loss(general_adaptive_loss_fn(severity=0.05), id)\n",
        "config1 = Configuration(cifar10_c_data, alexnet,noise_model, adaptive_loss_severity, identity_attack, model_pt, adaptive_loss_severity.id)\n",
        "configurations.append(config1)\n",
        "alexnet = AlexNet().to(device)\n",
        "id = f'CIFARC10:Alexnet:CE_LOSS'\n",
        "ce_loss = Loss(ce_loss_fn, id)\n",
        "baseline = Configuration(cifar10_c_data, alexnet,noise_model, ce_loss, identity_attack, id=ce_loss.id)\n",
        "configurations.append(baseline)\n",
        "ce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JxWnVITjdQcn",
        "outputId": "90f8363f-a397-4c87-e416-30d39196ce41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 0 / 312 Loss: 15.504997253417969\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 1 / 312 Loss: 15.469958305358887\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 2 / 312 Loss: 15.401481628417969\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 3 / 312 Loss: 15.296286582946777\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 4 / 312 Loss: 15.15048885345459\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 5 / 312 Loss: 14.954582214355469\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 6 / 312 Loss: 14.706832885742188\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 7 / 312 Loss: 14.415224075317383\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 8 / 312 Loss: 14.096799850463867\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 9 / 312 Loss: 13.786161422729492\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 10 / 312 Loss: 13.513849258422852\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 11 / 312 Loss: 13.29811954498291\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 12 / 312 Loss: 13.143203735351562\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 13 / 312 Loss: 13.035103797912598\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 14 / 312 Loss: 12.963563919067383\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 15 / 312 Loss: 12.916112899780273\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 16 / 312 Loss: 12.883627891540527\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 17 / 312 Loss: 12.862284660339355\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 18 / 312 Loss: 12.847259521484375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 19 / 312 Loss: 12.837261199951172\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 20 / 312 Loss: 12.829702377319336\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 21 / 312 Loss: 12.824378967285156\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 22 / 312 Loss: 12.820399284362793\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 23 / 312 Loss: 12.81732177734375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 24 / 312 Loss: 12.814973831176758\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 25 / 312 Loss: 12.813161849975586\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 26 / 312 Loss: 12.811725616455078\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 27 / 312 Loss: 12.810663223266602\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 28 / 312 Loss: 12.809797286987305\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 29 / 312 Loss: 12.80896282196045\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 30 / 312 Loss: 12.80833911895752\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 31 / 312 Loss: 12.80792236328125\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 32 / 312 Loss: 12.80744457244873\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 33 / 312 Loss: 12.80712890625\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 34 / 312 Loss: 12.806784629821777\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 35 / 312 Loss: 12.80645751953125\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 36 / 312 Loss: 12.80628776550293\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 37 / 312 Loss: 12.806122779846191\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 38 / 312 Loss: 12.805902481079102\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 39 / 312 Loss: 12.805784225463867\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 40 / 312 Loss: 12.805686950683594\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 41 / 312 Loss: 12.805546760559082\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 42 / 312 Loss: 12.805435180664062\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 43 / 312 Loss: 12.805315017700195\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 44 / 312 Loss: 12.805204391479492\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 45 / 312 Loss: 12.805154800415039\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 46 / 312 Loss: 12.805160522460938\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 47 / 312 Loss: 12.805057525634766\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 48 / 312 Loss: 12.805007934570312\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 49 / 312 Loss: 12.804977416992188\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 50 / 312 Loss: 12.804908752441406\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 51 / 312 Loss: 12.804872512817383\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 52 / 312 Loss: 12.804849624633789\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 53 / 312 Loss: 12.804811477661133\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 54 / 312 Loss: 12.804754257202148\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 55 / 312 Loss: 12.804763793945312\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 56 / 312 Loss: 12.804722785949707\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 57 / 312 Loss: 12.804731369018555\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 58 / 312 Loss: 12.80470085144043\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 59 / 312 Loss: 12.80467414855957\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 60 / 312 Loss: 12.804633140563965\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 61 / 312 Loss: 12.804616928100586\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 62 / 312 Loss: 12.804616928100586\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 63 / 312 Loss: 12.80456829071045\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 64 / 312 Loss: 12.804576873779297\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 65 / 312 Loss: 12.804590225219727\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 66 / 312 Loss: 12.804572105407715\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 67 / 312 Loss: 12.80458927154541\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 68 / 312 Loss: 12.804529190063477\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 69 / 312 Loss: 12.804546356201172\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 70 / 312 Loss: 12.804479598999023\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 71 / 312 Loss: 12.804487228393555\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 72 / 312 Loss: 12.804521560668945\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 73 / 312 Loss: 12.804451942443848\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 74 / 312 Loss: 12.804483413696289\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 75 / 312 Loss: 12.80445671081543\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 76 / 312 Loss: 12.804471969604492\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 77 / 312 Loss: 12.80445671081543\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 78 / 312 Loss: 12.804454803466797\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 79 / 312 Loss: 12.804423332214355\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 80 / 312 Loss: 12.804462432861328\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 81 / 312 Loss: 12.80441665649414\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 82 / 312 Loss: 12.804405212402344\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 83 / 312 Loss: 12.804397583007812\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 84 / 312 Loss: 12.804410934448242\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-77cd1a053b11>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3f466c6b009c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, noise_model, loss, config, epochs, eval_interval, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mnoise_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data, model, noise_model, loss, attack = configurations[0].getConfig()\n",
        "train(model,data, noise_model, loss, configurations[0], num_epoch, eval_interval, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}