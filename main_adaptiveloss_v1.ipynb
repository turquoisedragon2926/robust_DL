{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMU-5NWxiY-7",
        "outputId": "1cd67fd4-16e5-470e-82f2-edbaa3142102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/robust_DL\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/MyDrive/robust_DL\n",
        "# %mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JJOyIqHsaLcH"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from losses.trades import trades_loss\n",
        "import copy\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from models.wideresnet import *\n",
        "from models.resnet import *\n",
        "from models.small_cnn import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from models.AlexNet import AlexNet\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NdePmTVOdfv0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch import linalg\n",
        "\n",
        "\n",
        "def log_prob_drichlet(x,theta):\n",
        "    D = torch.distributions.dirichlet.Dirichlet(theta)\n",
        "    return D.log_prob(x)\n",
        "\n",
        "\n",
        "def gaussian_noise_layer(data, severity=0.05):\n",
        "    # c = [0.04, 0.06, .08, .09, .10][severity - 1]\n",
        "    noise = torch.randn(data.size())* severity#.to(device)\n",
        "\n",
        "    noise =torch.Tensor(noise).to(device)\n",
        "    # noise = noise*data.mean()\n",
        "    # noisy_data = torch.clip(data + noise)\n",
        "    noisy_data = data+noise\n",
        "    # noise = noisy_data - data\n",
        "    noise_norm = linalg.vector_norm(noise, dim = (1,2,3)).reshape((-1,1))  # improve this line of code later\n",
        "    return noisy_data, noise_norm\n",
        "\n",
        "def adaptive_loss(model,x_natural,y,noise_model, severity=0.05):\n",
        "    logits = model(x_natural)\n",
        "    p = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    noisy_data, noise_norm = gaussian_noise_layer(x_natural)\n",
        "    noise_weight = noise_model(noise_norm)#torch.div(noise_model(noise_norm), noise_model(torch.zeros_like(noise_norm)))\n",
        "    y_one_hot = torch.zeros_like(logits)\n",
        "    y_one_hot[:,y]=1\n",
        "\n",
        "    # loss_noisy = log_prob_dirichlet(logits, noise_weight)\n",
        "    loss_noisy = log_prob_drichlet(p,noise_weight*y_one_hot+1).mean()\n",
        "\n",
        "    # loss_normal = F.cross_entropy(logits , y)\n",
        "    return loss_noisy\n",
        "\n",
        "def adaptive_loss_v2(model,x_natural,y,noise_model, severity=0.05, w_noise=0.2):\n",
        "    logits = model(x_natural)\n",
        "    p = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    noisy_data, noise_norm = gaussian_noise_layer(x_natural)\n",
        "    noise_weight = noise_model(noise_norm)#torch.div(noise_model(noise_norm), noise_model(torch.zeros_like(noise_norm)))\n",
        "    y_one_hot = torch.zeros_like(logits)\n",
        "    y_one_hot[:,y]=1\n",
        "\n",
        "    # loss_noisy = log_prob_dirichlet(logits, noise_weight)\n",
        "    loss_noisy = log_prob_drichlet(p,noise_weight*y_one_hot+1).mean()\n",
        "\n",
        "    loss_normal = F.cross_entropy(logits , y)\n",
        "    return (1-w_noise)*loss_normal+w_noise*loss_noisy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E5VbKzmtaLcH"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "  def __init__(self, train_loader, valid_loader, test_loader, attack_loader):\n",
        "    self.train_loader = train_loader\n",
        "    self.valid_loader = valid_loader\n",
        "    self.test_loader = test_loader\n",
        "    self.attack_loader = attack_loader\n",
        "\n",
        "class Model:\n",
        "  model = None\n",
        "  def __init__(self, id):\n",
        "    self.id = id\n",
        "\n",
        "class Loss:\n",
        "  def __init__(self, loss_fn, id=None):\n",
        "    self.loss_fn = loss_fn\n",
        "    self.id = id\n",
        "\n",
        "class Configuration:\n",
        "  def __init__(self, data, model, noise_model, loss, attack, model_pt=None, id=None):\n",
        "    self.data = data\n",
        "    self.model = model\n",
        "    self.noise_model = noise_model\n",
        "    self.loss = loss\n",
        "    self.attack = attack\n",
        "    self.model_pt = model_pt # Should move this to model\n",
        "\n",
        "    self.id = id\n",
        "\n",
        "  def getConfig(self):\n",
        "    return self.data, self.model, self.noise_model, self.loss, self.attack\n",
        "\n",
        "  def getId(self):\n",
        "    return self.id\n",
        "\n",
        "class CIFAR10CDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "def general_trades_loss_fn(beta=6.0, epsilon=0.3, step_size=0.007, num_steps=10):\n",
        "  def trades_loss_fn(model, data, target, optimizer):\n",
        "    return trades_loss(model=model, x_natural=data, y=target, optimizer=optimizer, step_size=step_size,\n",
        "                      epsilon=epsilon, perturb_steps=num_steps, beta=beta, distance='l_inf')\n",
        "  return trades_loss_fn\n",
        "\n",
        "def general_adaptive_loss_fn(severity=0.05):\n",
        "  def adaptive_loss_fn(model, data, target,noise_model, severity):\n",
        "    return adaptive_loss_v2(model,data,target,noise_model, severity=severity)\n",
        "\n",
        "  return adaptive_loss_fn\n",
        "\n",
        "def ce_loss_fn(model, data, target, optimizer):\n",
        "    return F.cross_entropy(model(data), target)\n",
        "\n",
        "def identity_attack(model, X, y):\n",
        "  out = model(X)\n",
        "  acc = (out.data.max(1)[1] == y.data).float().sum()\n",
        "  return acc.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "emkusoN_aLcI"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, data_loader, device):\n",
        "    print('EVAL')\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(data_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "def robust_accuracy(model, attack, data_loader, device):\n",
        "    print('ROBUST EVAL')\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in data_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        X, y = Variable(data, requires_grad=True), Variable(target)\n",
        "        correct_count = attack(model, X, y)\n",
        "        correct += correct_count\n",
        "        total += target.size(0)\n",
        "    return 100. * correct / total\n",
        "\n",
        "def train(model, data,noise_model,  loss, config, epochs, eval_interval, device):\n",
        "  print('TRAINING')\n",
        "  data_loader = data.train_loader\n",
        "  valid_loader = data.valid_loader\n",
        "  attack_loader = data.attack_loader\n",
        "\n",
        "  optimizer = optim.SGD(list(model.parameters())+list(noise_model.parameters()), lr=0.01, momentum=0.9)\n",
        "  # TODO: We can move the optimizer to a field of Loss object\n",
        "\n",
        "  model.to(device)\n",
        "  if config.model_pt is not None:\n",
        "    model.load_state_dict(torch.load(config.model_pt))\n",
        "\n",
        "  best_eval_acc = 0.0\n",
        "  patience = 5  # number of VAL Acc values observed after best value to stop training\n",
        "\n",
        "  # Initialize lists to store per-epoch loss and validation accuracy\n",
        "  epoch_losses = []\n",
        "  eval_accuracies = []\n",
        "\n",
        "  for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    noise_model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(data_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        l = loss.loss_fn(model, data, target, noise_model, severity=0.05)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += l.item()\n",
        "\n",
        "        print(loss.id + f\" @ EP={epoch} & Batch idx \" + str(batch_idx) + \" / \" + str(len(data_loader) - 1) + \" Loss: \" + str(l.item()))\n",
        "\n",
        "    epoch_losses.append(total_loss / len(data_loader))\n",
        "\n",
        "    if epoch == 1 or epoch % eval_interval == 0 or epoch == epochs:\n",
        "      eval_acc= accuracy(model, valid_loader, device)\n",
        "      eval_accuracies.append(eval_acc)\n",
        "\n",
        "      if (eval_acc > best_eval_acc):  # best so far so save checkpoint to restore later\n",
        "        best_eval_acc = eval_acc\n",
        "        patience_count = 0\n",
        "        torch.save(model.state_dict(), os.path.join(\"weights\", loss.id + \".pt\"))\n",
        "        torch.save(optimizer.state_dict(), os.path.join(\"optimizers\", loss.id +  \".tar\"))\n",
        "      else:\n",
        "          patience_count += 1\n",
        "\n",
        "    # Plotting the loss and accuracy\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.suptitle(loss.id)\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epoch_losses, label='Training Loss')\n",
        "    plt.title('Loss vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot validation accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(eval_accuracies, label='Validation Accuracy')\n",
        "\n",
        "    if epoch == epochs or patience_count >= patience:\n",
        "      # Get the CIFAR 10 C evaluation accuracy and plot the horizontal line\n",
        "      cifar10c_eval_acc = robust_accuracy(model, config.attack, attack_loader, device)\n",
        "      plt.axhline(y=cifar10c_eval_acc, color='r', linestyle='-', label='CIFAR 10 C EVAL')\n",
        "\n",
        "    plt.title('Validation Accuracy vs. Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plots\n",
        "    if not os.path.exists('plots'):\n",
        "        os.makedirs('plots')\n",
        "    plt.savefig(os.path.join('plots', loss.id + '_training_validation_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "    if patience_count >= patience:\n",
        "      print(f\"Early Stopping!, epoch {epoch}\")\n",
        "      break\n",
        "\n",
        "  return total_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxEBl4HpaLcI",
        "outputId": "916ab7f5-db10-4138-bfcd-2a037460a48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "num_epoch=2\n",
        "valid_size=0.2\n",
        "eval_interval=1\n",
        "transform_train = transforms.Compose([\n",
        "transforms.RandomCrop(32, padding=4),\n",
        "transforms.RandomHorizontalFlip(),\n",
        "transforms.ToTensor(),])\n",
        "transform_test = transforms.Compose([\n",
        "transforms.ToTensor(),])\n",
        "use_cuda = torch.cuda.is_available()\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "transform_attack = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform_train)\n",
        "validset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform_test)\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "\n",
        "cifar10_train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, sampler=train_sampler, **kwargs)\n",
        "cifar10_valid_loader = torch.utils.data.DataLoader(trainset , batch_size=128, sampler=valid_sampler, **kwargs)\n",
        "cifar10_test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, **kwargs)\n",
        "\n",
        "\n",
        "transform_cifar10c = transforms.Compose([\n",
        "transforms.ToPILImage(),\n",
        "transforms.ToTensor(),\n",
        "transforms.Normalize((0.5,), (0.5,))])\n",
        "images = np.load('data/CIFAR-10-C/spatter.npy') # Set this to whatever we want\n",
        "labels = np.load('data/CIFAR-10-C/labels.npy')\n",
        "cifar10c_dataset = CIFAR10CDataset(data=images,labels=labels,transform=transform_cifar10c)\n",
        "cifar10c_attack_loader = DataLoader(cifar10c_dataset, batch_size=200, shuffle=False)\n",
        "\n",
        "cifar10_c_data = Data(cifar10_train_loader, cifar10_valid_loader,cifar10_test_loader, cifar10c_attack_loader)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "configurations = []\n",
        "final_loss = {}\n",
        "natural_accuracy = {}\n",
        "robustness_accuracy = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQLOW38MaLcI",
        "outputId": "7f5e5de7-56a0-4417-cfae-b1a3bfc82c94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Loss at 0x7af7c2725cc0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "alpha=5\n",
        "beta = 1 / alpha\n",
        "severity=0.05\n",
        "id = f'CIFARC10:Alexnet:TRADES_LOSS:BETA={beta}'\n",
        "model_pt = None # os.path.join(\"weights\", f'CIFARC10:RESNET18:TRADES_LOSS:BETA={beta}_ep=0.pt')\n",
        "\n",
        "alexnet = AlexNet().to(device)\n",
        "noise_model = nn.Sequential(nn.Linear(1,1, bias=True),nn.Sigmoid()).to(device)\n",
        "trades_loss_beta = Loss(general_trades_loss_fn(beta=beta), id)\n",
        "adaptive_loss_severity=Loss(general_adaptive_loss_fn(severity=0.05), id)\n",
        "config1 = Configuration(cifar10_c_data, alexnet,noise_model, adaptive_loss_severity, identity_attack, model_pt, adaptive_loss_severity.id)\n",
        "configurations.append(config1)\n",
        "alexnet = AlexNet().to(device)\n",
        "id = f'CIFARC10:Alexnet:CE_LOSS'\n",
        "ce_loss = Loss(ce_loss_fn, id)\n",
        "baseline = Configuration(cifar10_c_data, alexnet,noise_model, ce_loss, identity_attack, id=ce_loss.id)\n",
        "configurations.append(baseline)\n",
        "ce_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JxWnVITjdQcn",
        "outputId": "e19b739b-0a30-457f-f9f0-2028e12e8edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 0 / 312 Loss: 4.994904518127441\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 1 / 312 Loss: 4.994645595550537\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 2 / 312 Loss: 4.99257230758667\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 3 / 312 Loss: 4.991655349731445\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 4 / 312 Loss: 4.987752914428711\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 5 / 312 Loss: 4.984207630157471\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 6 / 312 Loss: 4.981136798858643\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 7 / 312 Loss: 4.975709438323975\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 8 / 312 Loss: 4.971420764923096\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 9 / 312 Loss: 4.965481281280518\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 10 / 312 Loss: 4.957733631134033\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 11 / 312 Loss: 4.950259208679199\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 12 / 312 Loss: 4.94347620010376\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 13 / 312 Loss: 4.933516502380371\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 14 / 312 Loss: 4.924062728881836\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 15 / 312 Loss: 4.913886070251465\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 16 / 312 Loss: 4.903982639312744\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 17 / 312 Loss: 4.8900556564331055\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 18 / 312 Loss: 4.876837730407715\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 19 / 312 Loss: 4.862154960632324\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 20 / 312 Loss: 4.84690523147583\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 21 / 312 Loss: 4.828395366668701\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 22 / 312 Loss: 4.811511039733887\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 23 / 312 Loss: 4.793764591217041\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 24 / 312 Loss: 4.771923065185547\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 25 / 312 Loss: 4.7532525062561035\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 26 / 312 Loss: 4.730574131011963\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 27 / 312 Loss: 4.709855079650879\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 28 / 312 Loss: 4.68825626373291\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 29 / 312 Loss: 4.666522026062012\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 30 / 312 Loss: 4.647140026092529\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 31 / 312 Loss: 4.625312328338623\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 32 / 312 Loss: 4.606048583984375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 33 / 312 Loss: 4.590576171875\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 34 / 312 Loss: 4.569867134094238\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 35 / 312 Loss: 4.555578231811523\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 36 / 312 Loss: 4.5402140617370605\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 37 / 312 Loss: 4.525763034820557\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 38 / 312 Loss: 4.516353607177734\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 39 / 312 Loss: 4.506735801696777\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 40 / 312 Loss: 4.495211124420166\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 41 / 312 Loss: 4.491554260253906\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 42 / 312 Loss: 4.481232166290283\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 43 / 312 Loss: 4.475196838378906\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 44 / 312 Loss: 4.467658042907715\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 45 / 312 Loss: 4.4630045890808105\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 46 / 312 Loss: 4.458920478820801\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 47 / 312 Loss: 4.455657958984375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 48 / 312 Loss: 4.451674938201904\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 49 / 312 Loss: 4.451416015625\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 50 / 312 Loss: 4.445602893829346\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 51 / 312 Loss: 4.442962646484375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 52 / 312 Loss: 4.44189453125\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 53 / 312 Loss: 4.4401068687438965\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 54 / 312 Loss: 4.439116954803467\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 55 / 312 Loss: 4.436983108520508\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 56 / 312 Loss: 4.432807922363281\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 57 / 312 Loss: 4.432604789733887\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 58 / 312 Loss: 4.430279731750488\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 59 / 312 Loss: 4.426957130432129\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 60 / 312 Loss: 4.426931381225586\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 61 / 312 Loss: 4.426998615264893\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 62 / 312 Loss: 4.428441524505615\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 63 / 312 Loss: 4.425882339477539\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 64 / 312 Loss: 4.42455530166626\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 65 / 312 Loss: 4.425232410430908\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 66 / 312 Loss: 4.4238128662109375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 67 / 312 Loss: 4.422910690307617\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 68 / 312 Loss: 4.4243059158325195\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 69 / 312 Loss: 4.420538425445557\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 70 / 312 Loss: 4.419137954711914\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 71 / 312 Loss: 4.420859336853027\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 72 / 312 Loss: 4.422016143798828\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 73 / 312 Loss: 4.421651840209961\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 74 / 312 Loss: 4.419085502624512\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 75 / 312 Loss: 4.4197916984558105\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 76 / 312 Loss: 4.418170928955078\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 77 / 312 Loss: 4.4188995361328125\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 78 / 312 Loss: 4.4174485206604\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 79 / 312 Loss: 4.418519496917725\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 80 / 312 Loss: 4.416170120239258\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 81 / 312 Loss: 4.416304111480713\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 82 / 312 Loss: 4.417428016662598\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 83 / 312 Loss: 4.417117595672607\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 84 / 312 Loss: 4.417327880859375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 85 / 312 Loss: 4.414656162261963\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 86 / 312 Loss: 4.414928436279297\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 87 / 312 Loss: 4.415894031524658\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 88 / 312 Loss: 4.41594934463501\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 89 / 312 Loss: 4.413857936859131\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 90 / 312 Loss: 4.415916442871094\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 91 / 312 Loss: 4.412689685821533\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 92 / 312 Loss: 4.414531707763672\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 93 / 312 Loss: 4.414542198181152\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 94 / 312 Loss: 4.415172100067139\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 95 / 312 Loss: 4.4164581298828125\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 96 / 312 Loss: 4.413990497589111\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 97 / 312 Loss: 4.413639545440674\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 98 / 312 Loss: 4.414433479309082\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 99 / 312 Loss: 4.408958911895752\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 100 / 312 Loss: 4.412679195404053\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 101 / 312 Loss: 4.414981842041016\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 102 / 312 Loss: 4.410621643066406\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 103 / 312 Loss: 4.4135050773620605\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 104 / 312 Loss: 4.413440227508545\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 105 / 312 Loss: 4.414428234100342\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 106 / 312 Loss: 4.415660858154297\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 107 / 312 Loss: 4.414725303649902\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 108 / 312 Loss: 4.413990020751953\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 109 / 312 Loss: 4.413012504577637\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 110 / 312 Loss: 4.412984848022461\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 111 / 312 Loss: 4.412062644958496\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 112 / 312 Loss: 4.414142608642578\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 113 / 312 Loss: 4.4137067794799805\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 114 / 312 Loss: 4.4159321784973145\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 115 / 312 Loss: 4.412744522094727\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 116 / 312 Loss: 4.411299228668213\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 117 / 312 Loss: 4.411131381988525\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 118 / 312 Loss: 4.414302825927734\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 119 / 312 Loss: 4.409639358520508\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 120 / 312 Loss: 4.411652565002441\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 121 / 312 Loss: 4.411574363708496\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 122 / 312 Loss: 4.412918567657471\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 123 / 312 Loss: 4.4117021560668945\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 124 / 312 Loss: 4.411614894866943\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 125 / 312 Loss: 4.409135818481445\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 126 / 312 Loss: 4.410170555114746\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 127 / 312 Loss: 4.411129951477051\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 128 / 312 Loss: 4.411937236785889\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 129 / 312 Loss: 4.411494731903076\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 130 / 312 Loss: 4.410652160644531\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 131 / 312 Loss: 4.414484977722168\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 132 / 312 Loss: 4.411904335021973\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 133 / 312 Loss: 4.412927150726318\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 134 / 312 Loss: 4.411544322967529\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 135 / 312 Loss: 4.409893035888672\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 136 / 312 Loss: 4.412425994873047\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 137 / 312 Loss: 4.409482479095459\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 138 / 312 Loss: 4.411860466003418\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 139 / 312 Loss: 4.4107465744018555\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 140 / 312 Loss: 4.411280632019043\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 141 / 312 Loss: 4.413148403167725\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 142 / 312 Loss: 4.4109907150268555\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 143 / 312 Loss: 4.412741184234619\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 144 / 312 Loss: 4.410928726196289\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 145 / 312 Loss: 4.408507347106934\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 146 / 312 Loss: 4.413150787353516\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 147 / 312 Loss: 4.408062934875488\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 148 / 312 Loss: 4.410553932189941\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 149 / 312 Loss: 4.412046432495117\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 150 / 312 Loss: 4.41232967376709\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 151 / 312 Loss: 4.410005569458008\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 152 / 312 Loss: 4.40902853012085\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 153 / 312 Loss: 4.407703399658203\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 154 / 312 Loss: 4.409686088562012\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 155 / 312 Loss: 4.407842636108398\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 156 / 312 Loss: 4.409652233123779\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 157 / 312 Loss: 4.4088640213012695\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 158 / 312 Loss: 4.40908145904541\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 159 / 312 Loss: 4.409057140350342\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 160 / 312 Loss: 4.40842866897583\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 161 / 312 Loss: 4.409115791320801\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 162 / 312 Loss: 4.4094390869140625\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 163 / 312 Loss: 4.4102020263671875\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 164 / 312 Loss: 4.411065578460693\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 165 / 312 Loss: 4.4105730056762695\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 166 / 312 Loss: 4.407494068145752\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 167 / 312 Loss: 4.409276962280273\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 168 / 312 Loss: 4.407108306884766\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 169 / 312 Loss: 4.409968852996826\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 170 / 312 Loss: 4.408544063568115\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 171 / 312 Loss: 4.410068511962891\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 172 / 312 Loss: 4.405200004577637\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 173 / 312 Loss: 4.410333633422852\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 174 / 312 Loss: 4.408346176147461\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 175 / 312 Loss: 4.409303188323975\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 176 / 312 Loss: 4.409757614135742\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 177 / 312 Loss: 4.4081220626831055\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 178 / 312 Loss: 4.408074855804443\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 179 / 312 Loss: 4.408450126647949\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 180 / 312 Loss: 4.408672332763672\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 181 / 312 Loss: 4.408814430236816\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 182 / 312 Loss: 4.409031867980957\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 183 / 312 Loss: 4.408604621887207\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 184 / 312 Loss: 4.407924652099609\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 185 / 312 Loss: 4.407252311706543\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 186 / 312 Loss: 4.410276412963867\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 187 / 312 Loss: 4.407036781311035\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 188 / 312 Loss: 4.409236907958984\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 189 / 312 Loss: 4.407057762145996\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 190 / 312 Loss: 4.408493518829346\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 191 / 312 Loss: 4.4097089767456055\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 192 / 312 Loss: 4.407658576965332\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 193 / 312 Loss: 4.410668849945068\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 194 / 312 Loss: 4.410964012145996\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 195 / 312 Loss: 4.406978130340576\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 196 / 312 Loss: 4.407575607299805\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 197 / 312 Loss: 4.4071550369262695\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 198 / 312 Loss: 4.407495975494385\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 199 / 312 Loss: 4.407297134399414\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 200 / 312 Loss: 4.407886028289795\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 201 / 312 Loss: 4.409360408782959\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 202 / 312 Loss: 4.40843391418457\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 203 / 312 Loss: 4.409855365753174\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 204 / 312 Loss: 4.407606601715088\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 205 / 312 Loss: 4.408554553985596\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 206 / 312 Loss: 4.406623363494873\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 207 / 312 Loss: 4.4070000648498535\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 208 / 312 Loss: 4.407344341278076\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 209 / 312 Loss: 4.407024383544922\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 210 / 312 Loss: 4.409480094909668\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 211 / 312 Loss: 4.407812118530273\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 212 / 312 Loss: 4.407439231872559\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 213 / 312 Loss: 4.406726837158203\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 214 / 312 Loss: 4.407561779022217\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 215 / 312 Loss: 4.407604217529297\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 216 / 312 Loss: 4.407126426696777\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 217 / 312 Loss: 4.406645774841309\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 218 / 312 Loss: 4.4069108963012695\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 219 / 312 Loss: 4.408065319061279\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 220 / 312 Loss: 4.409712314605713\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 221 / 312 Loss: 4.407048225402832\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 222 / 312 Loss: 4.407827377319336\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 223 / 312 Loss: 4.408146381378174\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 224 / 312 Loss: 4.406656742095947\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 225 / 312 Loss: 4.40634298324585\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 226 / 312 Loss: 4.40705680847168\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 227 / 312 Loss: 4.405760765075684\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 228 / 312 Loss: 4.406107425689697\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 229 / 312 Loss: 4.40521240234375\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 230 / 312 Loss: 4.408600807189941\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 231 / 312 Loss: 4.407464981079102\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 232 / 312 Loss: 4.408265113830566\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 233 / 312 Loss: 4.408600807189941\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 234 / 312 Loss: 4.406482696533203\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 235 / 312 Loss: 4.407858848571777\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 236 / 312 Loss: 4.407444000244141\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 237 / 312 Loss: 4.405303001403809\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 238 / 312 Loss: 4.405978202819824\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 239 / 312 Loss: 4.4075846672058105\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 240 / 312 Loss: 4.408053398132324\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 241 / 312 Loss: 4.4100518226623535\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 242 / 312 Loss: 4.407002925872803\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 243 / 312 Loss: 4.406852722167969\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 244 / 312 Loss: 4.409074306488037\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 245 / 312 Loss: 4.4057207107543945\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 246 / 312 Loss: 4.407310485839844\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 247 / 312 Loss: 4.406214237213135\n",
            "CIFARC10:Alexnet:TRADES_LOSS:BETA=0.2 @ EP=1 & Batch idx 248 / 312 Loss: 4.405629634857178\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-77cd1a053b11>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigurations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-3f466c6b009c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, noise_model, loss, config, epochs, eval_interval, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mnoise_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "data, model, noise_model, loss, attack = configurations[0].getConfig()\n",
        "train(model,data, noise_model, loss, configurations[0], num_epoch, eval_interval, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}